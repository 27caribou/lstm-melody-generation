{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from ipynb.fs.full.preprocess import generate_training_sequences, SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_UNITS = 38 # Must be the same size as vocabulary size\n",
    "NUM_UNITS = [256] # num_units = # neurons in internal layers of network\n",
    "LOSS = \"sparse_categorical_crossentropy\"\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5 #50 based on previous work\n",
    "BATCH_SIZE = 64\n",
    "SAVE_MODEL_PATH = \"model.h5\"\n",
    "\n",
    "\n",
    "def build_model(output_units, num_units, loss, learning_rate):\n",
    "    \n",
    "    # Create model architecture\n",
    "    \n",
    "    # First value None enables us to have as many timestamps as we want; so we can generate melodies of whatever length\n",
    "    input_layer = keras.layers.Input(shape=(None, output_units))\n",
    "    # This syntax draws an arrow from input to the LSTM\n",
    "    x = keras.layers.LSTM(num_units[0])(input_layer)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Could add more but we are dealing with a simple model?\n",
    "    output = keras.layers.Dense(output_units, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = keras.Model(input_layer, output)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=loss, \n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate), \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(output_units=OUTPUT_UNITS, num_units=NUM_UNITS, loss=LOSS, learning_rate=LEARNING_RATE):\n",
    "    \n",
    "    # Generate the training sequences\n",
    "    inputs, targets = generate_training_sequences(SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Build the network\n",
    "    model = build_model(output_units, num_units, loss, learning_rate) \n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(inputs, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    \"\"\"\n",
    "    You will notice that there is no train-test split. In our case, we are not really interested in how the model behaves\n",
    "    on unseen data. All we want is to get something that works.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(SAVE_MODEL_PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 38)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               302080    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 38)                9766      \n",
      "=================================================================\n",
      "Total params: 311,846\n",
      "Trainable params: 311,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "5660/5660 [==============================] - 1746s 308ms/step - loss: 0.6878 - accuracy: 0.7881\n",
      "Epoch 2/5\n",
      "5660/5660 [==============================] - 1773s 313ms/step - loss: 0.5816 - accuracy: 0.8118\n",
      "Epoch 3/5\n",
      "5660/5660 [==============================] - 1900s 336ms/step - loss: 0.5419 - accuracy: 0.8249\n",
      "Epoch 4/5\n",
      "5660/5660 [==============================] - 2050s 362ms/step - loss: 0.5193 - accuracy: 0.8316\n",
      "Epoch 5/5\n",
      "5660/5660 [==============================] - 1514s 267ms/step - loss: 0.4976 - accuracy: 0.8377\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
